{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "fbb6174e-221c-4093-b64b-82d90a82465e",
      "cell_type": "code",
      "source": "                                             ### KNN & PCA Assignment ###",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0fba0b8a-2f60-4cf2-ba65-af5f0307c1e8",
      "cell_type": "code",
      "source": "### Theoretical:\n\n1. What is K-Nearest Neighbors (KNN) and how does it work?\n2. What is the difference between KNN Classification and KNN Regression?\n3. What is the role of the distance metric in KNN?\n4. What is the Curse of Dimensionality in KNN?\n5. How can we choose the best value of K in KNN?\n6. What are KD Tree and Ball Tree in KNN\n7. When should you use KD Tree vs. Ball Tree?\n8. What are the disadvantages of KNN?\n9. How does feature scaling affect KNN?\n10. What is PCA (Principal Component Analysis)?\n11. How does PCA work?\n12. What is the geometric intuition behind PCA?\n13. What is the difference between Feature Selection and Feature Extraction?\n14. What are Eigenvalues and Eigenvectors in PCA?\n15. How do you decide the number of components to keep in PCA?\n16. Can PCA be used for classification?\n17. What are the limitations of PCAP\n18. How do KNN and PCA complement each other?\n19. How does KNN handle missing values in a dataset?\n20. What are the key differences between PCA and Linear Discriminant Analysis (LDA)?\n\n### Practical\n\n21. Train a KNN Classifier on the Iris dataset and print model accuracy,\n22. Train a KNN Regressor on a synthetic dataset and evaluate using Mean Squared Error (MSE).\n23. Train a KNN Classifier using different distance metrics (Euclidean and Manhattan) and compare accuracy.\n24. Train a KNN Classifier with different values of K and visualize decision boundaries\n25. Apply Feature Scaling before training a KNN model and compare results with unscaled data.\n26. Train a PCA model on synthetic data and print the explained variance ratio for each component.\n27. Apply PCA before training a KNN Classifier and compare accuracy with and without PCA.\n28. Perform Hyperparameter Tuning on a KNN Classifier using GridSearchCV.\n29. Train a KNN Classifier and check the number of misclassified samples.\n30. Train a PCA model and visualize the cumulative explained variance.\n31. Train a KNN Classifier using different values of the weights parameter (uniform vs. distance) and compare accuracy.\n32. Train a KNN Regressor and analyze the effect of different K values on performance.\n33. Implement KNN Imputation for handling missing values in a dataset.\n34. Train a PCA model and visualize the data projection onto the first two principal components.\n35. Train a KNN Classifier using the KD Tree and Ball Tree algorithms and compare performance.\n36. Train a PCA model on a high-dimensional dataset and visualize the Scree plot.\n37. Train a KNN Classifier and evaluate performance using Precision, Recall, and Fl-Score.\n38. Train a PCA model and analyze the effect of different numbers of components on accuracy.\n39. Train a KNN Classifier with different leaf_size values and compare accuracy.\n40. Train a PCA model and visualize how data points are transformed before and after PCA.\n41. Train a KNN Classifier on a real-world dataset (Wine dataset) and print classification report.\n42. Train a KNN Regressor and analyze the effect of different distance metrics on prediction error.\n43. Train a KNN Classifier and evaluate using ROC-AUC score.\n44. Train a PCA model and visualize the variance captured by each principal component.\n45. Train a KNN Classifier and perform feature selection before training.\n46. Train a PCA model and visualize the data reconstruction error after reducing dimensions.\n47. Train a KNN Classifier and visualize the decision boundary.\n48. Train a PCA model and analyze the effect of different numbers of components on data variance.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "014a0b9d-5fb3-42c7-9da5-5022cd49b610",
      "cell_type": "code",
      "source": "Answer1:- KNN is a supervised learning algorithm that classifies new instances based on the majority vote of its k nearest neighbors. It works by finding the k most similar instances to the new instance and assigning the most common class label.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2394acc0-d762-4708-af53-76cd72b1df29",
      "cell_type": "code",
      "source": "Answer2:- KNN Classification predicts class labels, while KNN Regression predicts continuous values.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f8ca4b57-0ca3-4f2f-813a-dc0582812d0e",
      "cell_type": "code",
      "source": "Answer3:- The distance metric measures the similarity between instances. Common metrics include Euclidean, Manhattan, and Minkowski distances.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "defe35cc-c031-4dfc-80d2-d361f3d52976",
      "cell_type": "code",
      "source": "Answer4:- The Curse of Dimensionality refers to the increased difficulty of finding nearest neighbors in high-dimensional spaces.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "221a2233-13e7-4a15-9531-93cef3898653",
      "cell_type": "code",
      "source": "Answer5:- The best value of K can be chosen using cross-validation and grid search.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e03e3a20-c2b7-45b1-8457-a3d525312a6c",
      "cell_type": "code",
      "source": "Answer6:- KD Tree and Ball Tree are data structures used to efficiently search for nearest neighbors.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1f586f50-b814-439a-8abb-a3ef51b8feb6",
      "cell_type": "code",
      "source": "Answer7:- KD Tree is suitable for low-dimensional data, while Ball Tree is suitable for high-dimensional data.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b77215e1-9667-4ce2-8845-55e996b89cc2",
      "cell_type": "code",
      "source": "Answer8:- KNN is sensitive to noise, outliers, and irrelevant features. It can also be computationally expensive.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0fe371a1-49c7-44f7-b610-2a8c27d25d9e",
      "cell_type": "code",
      "source": "Answer9:- Feature scaling is essential for KNN, as it ensures that all features are on the same scale.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "14fbb559-a5ec-4f89-a10d-a0e8d77ff796",
      "cell_type": "code",
      "source": "Answer10:- PCA is a dimensionality reduction technique that transforms high-dimensional data into lower-dimensional data while retaining most of the variance.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "acad802d-aba3-457b-8fc4-bb0e597207ff",
      "cell_type": "code",
      "source": "Answer11:- PCA works by finding the principal components, which are the directions of maximum variance in the data.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9c996cf7-d61d-4cc0-95a8-c0bc1289455b",
      "cell_type": "code",
      "source": "Answer12:- PCA finds the axes that capture the most variance in the data, allowing for dimensionality reduction while preserving information.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cbe96793-5874-4f71-8ad4-bb4fa0755593",
      "cell_type": "code",
      "source": "Answer13:- Feature Selection selects a subset of the original features, while Feature Extraction transforms the original features into new features.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d194200d-f164-4454-a487-4ffd3bb15f0f",
      "cell_type": "code",
      "source": "Answer14:- Eigenvalues represent the amount of variance explained by each principal component, while Eigenvectors represent the directions of the principal components.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "50d86eae-4433-4cb2-8019-c8c298502430",
      "cell_type": "code",
      "source": "Answer15:- The number of components can be chosen based on the explained variance ratio or using cross-validation.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e6c446ad-5294-4b8f-b7cd-ec2e8b7d1684",
      "cell_type": "code",
      "source": "Answer16:- PCA is typically used for dimensionality reduction, but it can be used as a preprocessing step for classification.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6ac09020-8b32-45aa-865f-02ba7cba9f78",
      "cell_type": "code",
      "source": "Answer17:- PCA assumes linearity and can be sensitive to outliers.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d11d3ed0-2bb7-4e12-a90e-1530e44b7e4a",
      "cell_type": "code",
      "source": "Answer18:- KNN can benefit from PCA's dimensionality reduction, which can improve its performance and efficiency.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "60bd92a1-9c03-4ae5-8623-2716de2b057f",
      "cell_type": "code",
      "source": "Answer19:- KNN can handle missing values using imputation techniques, such as mean or median imputation.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "340775cf-479a-48d1-8e04-e2041153bf17",
      "cell_type": "code",
      "source": "Answer20:- PCA focuses on variance, while LDA focuses on class separability.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "51a6d109-940a-4e10-8128-30630ff959f9",
      "cell_type": "code",
      "source": "Answer21:- from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f7d3f4a3-55e1-4249-9956-0a19d6a7a4c1",
      "cell_type": "code",
      "source": "Answer22:- import numpy as np\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\n\nX = np.random.rand(100, 1)\ny = 3 * X.squeeze() + 2 + np.random.randn(100)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nknn = KNeighborsRegressor(n_neighbors=5)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nprint(\"MSE:\", mean_squared_error(y_test, y_pred))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2cc8601d-8f7a-4530-a60c-f7ae818f5c2a",
      "cell_type": "code",
      "source": "Answer23:- from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n\nknn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\nknn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n\nknn_euclidean.fit(X_train, y_train)\nknn_manhattan.fit(X_train, y_train)\n\ny_pred_euclidean = knn_euclidean.predict(X_test)\ny_pred_manhattan = knn_manhattan.predict(X_test)\n\nprint(\"Euclidean Accuracy:\", accuracy_score(y_test, y_pred_euclidean))\nprint(\"Manhattan Accuracy:\", accuracy_score(y_test, y_pred_manhattan))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2f5f1e2d-1a07-483e-a95a-48a3a213c804",
      "cell_type": "code",
      "source": "Answer24:- from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nimport matplotlib.pyplot as plt\nimport numpy as np\n\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris.data[:, :2], iris.target, test_size=0.2, random_state=42)\n\nfor k in range(1, 6):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n\n    x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n    y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(xx, yy, Z, alpha=0.4)\n    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n    plt.title(f\"KNN with k={k}\")\n    plt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8285ebdf-693b-4815-b58b-c4243e04c546",
      "cell_type": "code",
      "source": "Answer25:- from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n\nknn_unscaled = KNeighborsClassifier(n_neighbors=5)\nknn_unscaled.fit(X_train, y_train)\ny_pred_unscaled = knn_unscaled.predict(X_test)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nknn_scaled = KNeighborsClassifier(n_neighbors=5)\nknn_scaled.fit(X_train_scaled, y_train)\ny_pred_scaled = knn_scaled.predict(X_test_scaled)\n\nprint(\"Unscaled Accuracy:\", accuracy_score(y_test, y_pred_unscaled))\nprint(\"Scaled Accuracy:\", accuracy_score(y_test, y_pred_scaled))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0b00940a-b96d-4939-b9e8-3f5ebc167269",
      "cell_type": "code",
      "source": "Answer26:- import numpy as np\nfrom sklearn.decomposition import PCA\n\nnp.random.seed(0)\nX = np.random.rand(100, 5)\n\npca = PCA(n_components=5)\npca.fit(X)\n\nprint(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6e9d8e74-a6c1-4add-b618-413f0391609d",
      "cell_type": "code",
      "source": "Answer27:- from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n\nknn_without_pca = KNeighborsClassifier(n_neighbors=5)\nknn_without_pca.fit(X_train, y_train)\ny_pred_without_pca = knn_without_pca.predict(X_test)\n\npca = PCA(n_components=2)\nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.transform(X_test)\n\nknn_with_pca = KNeighborsClassifier(n_neighbors=5)\nknn_with_pca.fit(X_train_pca, y_train)\ny_pred_with_pca = knn_with_pca.predict(X_test_pca)\n\nprint(\"Accuracy without PCA:\", accuracy_score(y_test, y_pred_without_pca))\nprint(\"Accuracy with PCA:\", accuracy_score(y_test, y_pred_with_pca))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ea593b9a-7c80-4e57-96b4-480c10c0e90e",
      "cell_type": "code",
      "source": "Answer28:- from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\n\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n\nparam_grid = {'n_neighbors': range(1, 11), 'weights': ['uniform', 'distance']}\nknn = KNeighborsClassifier()\ngrid_search = GridSearchCV(knn, param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\n\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Best Score:\", grid_search.best_score_)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f03ced16-225b-45a8-b0de-71ec89cb6fc9",
      "cell_type": "code",
      "source": "Answer29:- from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\nmisclassified_samples = (y_test != y_pred).sum()\nprint(\"Number of Misclassified Samples:\", misclassified_samples)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8df83e07-7a80-419f-915a-1494991e8946",
      "cell_type": "code",
      "source": "Answer30:- import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)\nX = np.random.rand(100, 5)\n\npca = PCA(n_components=5)\npca.fit(X)\n\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel(\"Number of Components\")\nplt.ylabel(\"Cumulative Explained Variance\")\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cde9fa20-c81a-493e-82f4-7caafde9e7ed",
      "cell_type": "code",
      "source": "Answer31:- from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n\nknn_uniform = KNeighborsClassifier(n_neighbors=5, weights='uniform')\nknn_distance = KNeighborsClassifier(n_neighbors=5, weights='distance')\n\nknn_uniform.fit(X_train, y_train)\nknn_distance.fit(X_train, y_train)\n\ny_pred_uniform = knn_uniform.predict(X_test)\ny_pred_distance = knn_distance.predict(X_test)\n\nprint(\"Uniform Weights Accuracy:\", accuracy_score(y_test, y_pred_uniform))\nprint(\"Distance Weights Accuracy:\", accuracy_score(y_test, y_pred_distance))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "03c69f1d-a6e6-45db-9eb1-224f5a91348a",
      "cell_type": "code",
      "source": "Answer32:- import numpy as np\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\n\nnp.random.seed(0)\nX = np.random.rand(100, 1)\ny = 3 * X.squeeze() + 2 + np.random.randn(100)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmse_values = []\nfor k in range(1, 11):\n    knn = KNeighborsRegressor(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    mse_values.append(mean_squared_error(y_test, y_pred))\n\nplt.plot(range(1, 11), mse_values)\nplt.xlabel(\"K Value\")\nplt.ylabel(\"Mean Squared Error\")\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "92c1664a-79c4-4f41-a08b-5a7430cf772e",
      "cell_type": "code",
      "source": "Answer33:- import numpy as np\nfrom sklearn.impute import KNNImputer\n\nnp.random.seed(0)\nX = np.random.rand(100, 5)\nX_missing = X.copy()\nX_missing[np.random.choice(100, 10), np.random.choice(5, 10)] = np.nan\n\nimputer = KNNImputer(n_neighbors=5)\nX_imputed = imputer.fit_transform(X_missing)\n\nprint(\"Original Data:\")\nprint(X)\nprint(\"Imputed Data:\")\nprint(X_imputed)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2b8b987a-7714-4bf0-bf2f-872fb948f31a",
      "cell_type": "code",
      "source": "Answer34:- import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)\nX = np.random.rand(100, 5)\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nplt.scatter(X_pca[:, 0], X_pca[:, 1])\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0da8ee19-a1f2-4d16-b7db-183aeabc1aca",
      "cell_type": "code",
      "source": "Answer35:- from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n\nknn_kdtree = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree')\nknn_balltree = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')\n\nknn_kdtree.fit(X_train, y_train)\nknn_balltree.fit(X_train, y_train)\n\ny_pred_kdtree = knn_kdtree.predict(X_test)\ny_pred_balltree = knn_balltree.predict(X_test)\n\nprint(\"KD Tree Accuracy:\", accuracy_score(y_test, y_pred_kdtree))\nprint(\"Ball Tree Accuracy:\", accuracy_score(y_test, y_pred_balltree))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "74da7246-2f79-4916-9d7e-fb53a59061ae",
      "cell_type": "code",
      "source": "Answer36:- import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)\nX = np.random.rand(100, 10)\n\npca = PCA(n_components=10)\npca.fit(X)\n\nplt.plot(pca.explained_variance_ratio_)\nplt.xlabel(\"Principal Component\")\nplt.ylabel(\"Explained Variance Ratio\")\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b396d7c5-ebb6-49a1-b902-978149d542d8",
      "cell_type": "code",
      "source": "Answer37:- from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\nprint(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\nprint(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\nprint(\"F1-Score:\", f1_score(y_test, y_pred, average='weighted'))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "aebd8e3f-1e26-4add-a3c5-bb9985ff3568",
      "cell_type": "code",
      "source": "Answer38:- from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n\naccuracy_values = []\nfor n_components in range(1, 5):\n    pca = PCA(n_components=n_components)\n    X_train_pca = pca.fit_transform(X_train)\n    X_test_pca = pca.transform(X_test)\n\n    knn = KNeighborsClassifier(n_neighbors=5)\n    knn.fit(X_train_pca, y_train)\n    y_pred = knn.predict(X_test_pca)\n\n    accuracy_values.append(accuracy_score(y_test, y_pred))\n\nplt.plot(range(1, 5), accuracy_values)\nplt.xlabel(\"Number of Components\")\nplt.ylabel(\"Accuracy\")\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "346b83d6-0556-4137-96d6-382d1f65fd5e",
      "cell_type": "code",
      "source": "Answer39:- from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n\naccuracy_values = []\nfor leaf_size in [10, 20, 30]:\n    knn = KNeighborsClassifier(n_neighbors=5, leaf_size=leaf_size)\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    accuracy_values.append(accuracy_score(y_test, y_pred))\n\nplt.plot([10, 20, 30], accuracy_values)\nplt.xlabel(\"Leaf Size\")\nplt.ylabel(\"Accuracy\")\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d9ea98cf-dcf5-4754-97ea-e2cc6d1c8bd6",
      "cell_type": "code",
      "source": "Answer40:- import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.scatter(X[:, 0], X[:, 1], c=y)\nplt.title(\"Original Data\")\n\nplt.subplot(1, 2, 2)\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)\nplt.title(\"Data after PCA\")\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0309b8ff-fbdc-41a5-a26e-0fdd94ec2232",
      "cell_type": "code",
      "source": "Answer41:- from sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report\n\nwine = load_wine()\nX_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.2, random_state=42)\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\nprint(classification_report(y_test, y_pred))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e9ccf9a1-7fed-4df4-9f8e-80410ca7f52e",
      "cell_type": "code",
      "source": "Answer42:- import numpy as np\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_regression(n_samples=100, n_features=5, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmse_values = []\nfor metric in ['euclidean', 'manhattan', 'minkowski']:\n    knn = KNeighborsRegressor(n_neighbors=5, metric=metric)\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    mse_values.append(mean_squared_error(y_test, y_pred))\n\nprint(\"MSE values for different distance metrics:\", mse_values)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b3420f7c-9e37-4fb3-a73a-8e92c7b88d05",
      "cell_type": "code",
      "source": "Answer43:- from sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import roc_auc_score\n\nbreast_cancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(breast_cancer.data, breast_cancer.target, test_size=0.2, random_state=42)\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\ny_pred_proba = knn.predict_proba(X_test)[:, 1]\n\nprint(\"ROC-AUC score:\", roc_auc_score(y_test, y_pred_proba))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1df71bfe-def9-42ce-a7e4-620de6392237",
      "cell_type": "code",
      "source": "Answer44:- import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data\n\npca = PCA(n_components=4)\npca.fit(X)\n\nplt.plot(pca.explained_variance_ratio_)\nplt.xlabel(\"Principal Component\")\nplt.ylabel(\"Variance Captured\")\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "97c06176-678f-4825-9141-cc587f3260ca",
      "cell_type": "code",
      "source": "Answer45:- from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n\nselector = SelectKBest(f_classif, k=2)\nX_train_selected = selector.fit_transform(X_train, y_train)\nX_test_selected = selector.transform(X_test)\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train_selected, y_train)\ny_pred = knn.predict(X_test_selected)\n\nprint(\"Accuracy:\", knn.score(X_test_selected, y_test))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "15c494ce-8616-4cef-8863-0befd630f41d",
      "cell_type": "code",
      "source": "Answer46:- import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data\n\npca = PCA(n_components=None)\npca.fit(X)\n\nreconstruction_errors = []\nfor n_components in range(1, X.shape[1] + 1):\n    pca = PCA(n_components=n_components)\n    X_reduced = pca.fit_transform(X)\n    X_reconstructed = pca.inverse_transform(X_reduced)\n    reconstruction_error = np.mean((X - X_reconstructed) ** 2)\n    reconstruction_errors.append(reconstruction_error)\n\nplt.plot(range(1, X.shape[1] + 1), reconstruction_errors)\nplt.xlabel(\"Number of Components\")\nplt.ylabel(\"Reconstruction Error\")\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ff9991d0-c533-4e4f-aa96-8a1828da2c8c",
      "cell_type": "code",
      "source": "Answer47:- import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X, y)\n\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.contourf(xx, yy, Z, alpha=0.4)\nplt.scatter(X[:, 0], X[:, 1], c=y)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e58fa974-d5fa-4475-a2a9-e6e6e62a0733",
      "cell_type": "code",
      "source": "Answer48:- import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data\n\npca = PCA(n_components=None)\npca.fit(X)\n\nexplained_variance_ratios = []\nfor n_components in range(1, X.shape[1] + 1):\n    pca = PCA(n_components=n_components)\n    pca.fit(X)\n    explained_variance_ratios.append(np.sum(pca.explained_variance_ratio_))\n\nplt.plot(range(1, X.shape[1] + 1), explained_variance_ratios)\nplt.xlabel(\"Number of Components\")\nplt.ylabel(\"Cumulative Explained Variance\")\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}